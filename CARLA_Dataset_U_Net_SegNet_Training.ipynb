{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Consolidated Dataset"
      ],
      "metadata": {
        "id": "sEGubGHuxwJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "AxTl2Q0yxwJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1fjRQBzft-amgePhi_1Hetq2QTSj3MPH-"
      ],
      "metadata": {
        "id": "HgdqtjZjxwJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfgHKet-xwJj"
      },
      "outputs": [],
      "source": [
        "! unzip -qq /content/consolidated-dataset.zip -d data/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/data/content/consolidated-dataset ."
      ],
      "metadata": {
        "id": "Tc54uLV9zmTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r consolidated-dataset.zip"
      ],
      "metadata": {
        "id": "3_Qs63L1xwJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r data"
      ],
      "metadata": {
        "id": "LcUtxW2N0Gko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4MDKUtPxwJk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def count_files_in_subfolders(directory):\n",
        "\n",
        "    file_counts = {}\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        # Count only files in each subdirectory, excluding the root directory\n",
        "        if root != directory:\n",
        "            file_counts[root] = len([f for f in files if os.path.isfile(os.path.join(root, f))])\n",
        "    return file_counts\n",
        "\n",
        "# Replace 'your_directory_path' with the path to your directory\n",
        "directory_path = 'consolidated-dataset'\n",
        "file_counts = count_files_in_subfolders(directory_path)\n",
        "\n",
        "# Printing the results\n",
        "for path, count in file_counts.items():\n",
        "    print(f\"{path}: {count} files\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## U-Net"
      ],
      "metadata": {
        "id": "3pMECsyoxwJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random split"
      ],
      "metadata": {
        "id": "fo2fA051xwJm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxKST4tWxwJm"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nay4Z6TexwJm"
      },
      "outputs": [],
      "source": [
        "base_dir = 'consolidated-dataset'\n",
        "all_towns = ['']  # Include all towns\n",
        "image_size = (192, 256)  # Example size, adjust as needed\n",
        "batch_size = 32  # Adjust based on your system's capability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPYzBLAYxwJn"
      },
      "outputs": [],
      "source": [
        "def get_file_paths(base_dir, towns):\n",
        "    image_paths, mask_paths = [], []\n",
        "    for town in towns:\n",
        "        images_dir = os.path.join(base_dir, town, 'Camera_RGB')\n",
        "        masks_dir = os.path.join(base_dir, town, 'Camera_SemSeg')\n",
        "\n",
        "        images = [os.path.join(images_dir, f) for f in os.listdir(images_dir) if f.endswith('.png')]\n",
        "        masks = [os.path.join(masks_dir, f) for f in os.listdir(masks_dir) if f.endswith('.png')]\n",
        "\n",
        "        image_paths.extend(images)\n",
        "        mask_paths.extend(masks)\n",
        "\n",
        "    return image_paths, mask_paths\n",
        "\n",
        "# Getting file paths for each set\n",
        "all_images, all_masks = get_file_paths(base_dir, all_towns)\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "train_ratio = 0.6\n",
        "validation_ratio = 0.2\n",
        "test_ratio = 0.2\n",
        "\n",
        "# First split to separate out the training set\n",
        "train_images, test_images, train_masks, test_masks = train_test_split(all_images, all_masks, test_size=1 - train_ratio, random_state = 9)\n",
        "\n",
        "# Second split to divide the remaining data into validation and test sets\n",
        "val_images, test_images, val_masks, test_masks = train_test_split(test_images, test_masks, test_size=test_ratio/(test_ratio + validation_ratio))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3RXqeaHxwJn"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_png(image, channels=3)\n",
        "    image = tf.image.resize(image, image_size)\n",
        "    image = image / 255.0  # Normalize to [0, 1]\n",
        "    return image\n",
        "\n",
        "def preprocess_mask(mask_path):\n",
        "    mask = tf.io.read_file(mask_path)\n",
        "    mask = tf.image.decode_png(mask, channels=3)  # Load as a 3-channel image\n",
        "    mask = tf.image.resize(mask, image_size, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\n",
        "    # Extract the first channel that contains class encoding\n",
        "    mask = mask[:, :, 0]\n",
        "\n",
        "\n",
        "    mask = (mask == 24)\n",
        "    mask = tf.cast(mask, tf.float32)  # Convert to float32\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "def load_and_preprocess(image_path, mask_path):\n",
        "    return preprocess_image(image_path), preprocess_mask(mask_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJYFPI8ZxwJn"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_masks))\n",
        "train_dataset = train_dataset.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_masks))\n",
        "val_dataset = val_dataset.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "val_dataset = val_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_masks))\n",
        "test_dataset = test_dataset.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(batch_size)\n",
        "test_dataset = test_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DL7oknc7xwJn"
      },
      "outputs": [],
      "source": [
        "def show_sample(dataset):\n",
        "    for images, masks in dataset.take(1):\n",
        "        plt.figure(figsize=(30, 10))\n",
        "        for i in range(5):\n",
        "            plt.subplot(2, 5, i + 1)\n",
        "            plt.imshow(images[i])\n",
        "            plt.axis('off')\n",
        "            plt.subplot(2, 5, i + 6)\n",
        "            plt.imshow(masks[i, :, :], cmap='gray')\n",
        "            plt.axis('off')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_sample(train_dataset)"
      ],
      "metadata": {
        "id": "Dnj7BUSixwJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_sample(val_dataset)"
      ],
      "metadata": {
        "id": "CWbOvPLnxwJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_sample(test_dataset)"
      ],
      "metadata": {
        "id": "NFG_h4P3xwJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dj2ualcQxwJn"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Lambda, RepeatVector, Reshape\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVALPl9ExwJn"
      },
      "outputs": [],
      "source": [
        "height = image_size[0]\n",
        "width = image_size[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9PvuPM3xwJn"
      },
      "outputs": [],
      "source": [
        "def dice_coefficient(y_true, y_pred, smooth=1):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "def iou(y_true, y_pred, smooth=1):\n",
        "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
        "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
        "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
        "    return jac\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSOeLSIrxwJo"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import BatchNormalization, Dropout, Activation\n",
        "\n",
        "def conv_block(input_tensor, num_filters):\n",
        "\n",
        "    # first layer\n",
        "    x = Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # second layer\n",
        "    x = Conv2D(num_filters, (3, 3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def encoder_block(input_tensor, num_filters):\n",
        "\n",
        "    x = conv_block(input_tensor, num_filters)\n",
        "    p = MaxPooling2D((2, 2))(x)\n",
        "    return x, p\n",
        "\n",
        "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
        "\n",
        "    x = Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
        "    x = concatenate([x, concat_tensor], axis=-1)\n",
        "    x = conv_block(x, num_filters)\n",
        "    return x\n",
        "\n",
        "input_img = Input((height, width, 3), name='img')\n",
        "\n",
        "# Encoder\n",
        "x1, p1 = encoder_block(input_img, 32)\n",
        "x2, p2 = encoder_block(p1, 64)\n",
        "x3, p3 = encoder_block(p2, 128)\n",
        "x4, p4 = encoder_block(p3, 256)\n",
        "\n",
        "# Bridge\n",
        "x5 = conv_block(p4, 512)\n",
        "\n",
        "# Decoder\n",
        "x6 = decoder_block(x5, x4, 256)\n",
        "x7 = decoder_block(x6, x3, 128)\n",
        "x8 = decoder_block(x7, x2, 64)\n",
        "x9 = decoder_block(x8, x1, 32)\n",
        "\n",
        "# Output\n",
        "outputs = Conv2D(1, (1, 1), activation='sigmoid')(x9)\n",
        "\n",
        "model = Model(inputs=[input_img], outputs=[outputs])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy', dice_coefficient, iou])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFOh2Qs9xwJo"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    EarlyStopping(patience=5, verbose=1),\n",
        "    ReduceLROnPlateau(patience=5, verbose=1),\n",
        "    ModelCheckpoint('model-192-unet.h5', verbose=1, save_best_only=True, monitor = 'val_iou', mode = 'max')\n",
        "]\n",
        "\n",
        "results = model.fit(train_dataset, epochs=20, callbacks=callbacks,\n",
        "                    validation_data=val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model (assuming it's saved as 'model-sdc-seg-v2_192.h5')\n",
        "model = tf.keras.models.load_model('model-192.h5', custom_objects={'dice_coefficient': dice_coefficient, 'iou': iou})\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_accuracy, test_dice, test_iou = model.evaluate(test_dataset)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "print(f\"Test Dice Coefficient: {test_dice}\")\n",
        "print(f\"Test IoU: {test_iou}\")"
      ],
      "metadata": {
        "id": "Z6YSfR2txwJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5NjMjuuxwJo"
      },
      "outputs": [],
      "source": [
        "def plot_predictions(dataset, model, num_examples=5):\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    for images, masks in dataset.take(1):\n",
        "        preds = model.predict(images)\n",
        "\n",
        "        for i in range(num_examples):\n",
        "            plt.subplot(3, num_examples, i + 1)\n",
        "            plt.imshow(images[i])\n",
        "            plt.title('Image')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(3, num_examples, num_examples + i + 1)\n",
        "            plt.imshow(masks[i, :, :], cmap='gray')\n",
        "            plt.title('True Mask')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(3, num_examples, 2 * num_examples + i + 1)\n",
        "            plt.imshow(preds[i].squeeze(), cmap='gray')\n",
        "            plt.title('Predicted Mask')\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot predictions\n",
        "plot_predictions(test_dataset, model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1ndiwxCG1XgfsN4RSsclApuXuQLihxKNZ"
      ],
      "metadata": {
        "id": "9HLrNfBOxwJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq surprise.zip"
      ],
      "metadata": {
        "id": "i11eGu9VxwJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_images, eval_masks = get_file_paths('surprise', [''])"
      ],
      "metadata": {
        "id": "SgLE12f1xwJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = tf.data.Dataset.from_tensor_slices((eval_images, eval_masks))\n",
        "test_dataset = test_dataset.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(batch_size)\n",
        "test_dataset = test_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "utvC3Z3nxwJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_accuracy, test_dice, test_iou = model.evaluate(test_dataset)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "print(f\"Test Dice Coefficient: {test_dice}\")\n",
        "print(f\"Test IoU: {test_iou}\")"
      ],
      "metadata": {
        "id": "3RnIfcvYxwJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seg-Net"
      ],
      "metadata": {
        "id": "72zTCo0xBRof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random split"
      ],
      "metadata": {
        "id": "uawn1Ff0BRol"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z43MVcjYBRol"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEcQsNjIBRol"
      },
      "outputs": [],
      "source": [
        "base_dir = 'consolidated-dataset'\n",
        "all_towns = ['']  # Include all towns\n",
        "image_size = (192, 256)  # Example size, adjust as needed\n",
        "batch_size = 16  # Adjust based on your system's capability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Iiw_zxABRol"
      },
      "outputs": [],
      "source": [
        "def get_file_paths(base_dir, towns):\n",
        "    image_paths, mask_paths = [], []\n",
        "    for town in towns:\n",
        "        images_dir = os.path.join(base_dir, town, 'Camera_RGB')\n",
        "        masks_dir = os.path.join(base_dir, town, 'Camera_SemSeg')\n",
        "\n",
        "        images = [os.path.join(images_dir, f) for f in os.listdir(images_dir) if f.endswith('.png')]\n",
        "        masks = [os.path.join(masks_dir, f) for f in os.listdir(masks_dir) if f.endswith('.png')]\n",
        "\n",
        "        image_paths.extend(images)\n",
        "        mask_paths.extend(masks)\n",
        "\n",
        "    return image_paths, mask_paths\n",
        "\n",
        "# Getting file paths for each set\n",
        "all_images, all_masks = get_file_paths(base_dir, all_towns)\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "train_ratio = 0.6\n",
        "validation_ratio = 0.2\n",
        "test_ratio = 0.2\n",
        "\n",
        "# First split to separate out the training set\n",
        "train_images, test_images, train_masks, test_masks = train_test_split(all_images, all_masks, test_size=1 - train_ratio)\n",
        "\n",
        "# Second split to divide the remaining data into validation and test sets\n",
        "val_images, test_images, val_masks, test_masks = train_test_split(test_images, test_masks, test_size=test_ratio/(test_ratio + validation_ratio))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zRd1I-lBRom"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_png(image, channels=3)\n",
        "    image = tf.image.resize(image, image_size)\n",
        "    image = image / 255.0  # Normalize to [0, 1]\n",
        "    return image\n",
        "\n",
        "def preprocess_mask(mask_path):\n",
        "    mask = tf.io.read_file(mask_path)\n",
        "    mask = tf.image.decode_png(mask, channels=3)  # Load as a 3-channel image\n",
        "    mask = tf.image.resize(mask, image_size, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\n",
        "    # Extract the first channel that contains class encoding\n",
        "    mask = mask[:, :, 0]\n",
        "\n",
        "\n",
        "    mask = (mask == 24)\n",
        "    mask = tf.cast(mask, tf.float32)  # Convert to float32\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "def load_and_preprocess(image_path, mask_path):\n",
        "    return preprocess_image(image_path), preprocess_mask(mask_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6WeKSjjBRom"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_masks))\n",
        "train_dataset = train_dataset.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_masks))\n",
        "val_dataset = val_dataset.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "val_dataset = val_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_masks))\n",
        "test_dataset = test_dataset.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(batch_size)\n",
        "test_dataset = test_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aTQJeeABRom"
      },
      "outputs": [],
      "source": [
        "def show_sample(dataset):\n",
        "    for images, masks in dataset.take(1):\n",
        "        plt.figure(figsize=(30, 10))\n",
        "        for i in range(5):\n",
        "            plt.subplot(2, 5, i + 1)\n",
        "            plt.imshow(images[i])\n",
        "            plt.axis('off')\n",
        "            plt.subplot(2, 5, i + 6)\n",
        "            plt.imshow(masks[i, :, :], cmap='gray')\n",
        "            plt.axis('off')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_sample(train_dataset)"
      ],
      "metadata": {
        "id": "bzmE8CjYBRom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_sample(val_dataset)"
      ],
      "metadata": {
        "id": "pjCkNU1-BRom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_sample(test_dataset)"
      ],
      "metadata": {
        "id": "aJUM2Fp2BRom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1B6Mf9HAVjGH90TCtBYEumB7OxiuMOjvU"
      ],
      "metadata": {
        "id": "FIW5dsJaG4G8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq core.zip"
      ],
      "metadata": {
        "id": "dhU61CtmG0GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72PRhdp3BRom"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Lambda, RepeatVector, Reshape\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, BatchNormalization, Activation\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras import backend as K\n",
        "from core.layers import MaxPoolingWithArgmax2D, MaxUnPooling2D\n",
        "from core.segnet import SegNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIH6OvxHBRom"
      },
      "outputs": [],
      "source": [
        "height = image_size[0]\n",
        "width = image_size[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZj6j7PnBRon"
      },
      "outputs": [],
      "source": [
        "def dice_coefficient(y_true, y_pred, smooth=1):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "def iou(y_true, y_pred, smooth=1):\n",
        "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
        "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
        "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
        "    return jac\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_block(input_tensor, num_filters):\n",
        "    x = input_tensor\n",
        "\n",
        "    for n_filter in num_filters:\n",
        "        x = Conv2D(n_filter, (3, 3), padding='same', kernel_initializer='he_normal')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "    x, mask = MaxPoolingWithArgmax2D(pool_size=(2, 2))(x)\n",
        "    return x, mask\n",
        "\n",
        "def decoder_block(input_tensor, mask, num_filters):\n",
        "    x = MaxUnPooling2D(size=(2, 2))([input_tensor, mask])\n",
        "\n",
        "    for n_filter in num_filters:\n",
        "        x = Conv2D(n_filter, (3, 3), padding='same', kernel_initializer='he_normal')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def build_segnet(input_shape):\n",
        "    input_img = Input(shape=input_shape, name='Image')\n",
        "\n",
        "    # Encoder\n",
        "    x1, m1 = encoder_block(input_img, [32, 32])\n",
        "    x2, m2 = encoder_block(x1, [64, 64])\n",
        "    x3, m3 = encoder_block(x2, [128, 128, 128])\n",
        "    x4, m4 = encoder_block(x3, [256, 256, 256])\n",
        "    x5, m5 = encoder_block(x4, [256, 256, 256])\n",
        "\n",
        "    # Decoder\n",
        "    x6 = decoder_block(x5, m5, [256, 256, 256])\n",
        "    x7 = decoder_block(x6, m4, [256, 256, 128])\n",
        "    x8 = decoder_block(x7, m3, [128, 128, 64])\n",
        "    x9 = decoder_block(x8, m2, [64, 32])\n",
        "    x10 = decoder_block(x9, m1, [32])\n",
        "\n",
        "    # Output\n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(x10)\n",
        "\n",
        "    return Model(inputs=[input_img], outputs=[outputs])\n",
        "\n",
        "height = image_size[0]\n",
        "width = image_size[1]\n",
        "\n",
        "model = build_segnet((height, width, 3))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', dice_coefficient, iou])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "4AM9vak4N3S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vuOgOWDBRon"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    EarlyStopping(patience=5, verbose=1),\n",
        "    ReduceLROnPlateau(patience=5, verbose=1),\n",
        "    ModelCheckpoint('model-192-segnet.h5', verbose=1, save_best_only=True, monitor = 'val_iou', mode = 'max')\n",
        "]\n",
        "\n",
        "results = model.fit(train_dataset, epochs=30, callbacks=callbacks,\n",
        "                    validation_data=val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model (assuming it's saved as 'model-sdc-seg-v2_192.h5')\n",
        "model = tf.keras.models.load_model('model-192-segnet.h5', custom_objects={'dice_coefficient': dice_coefficient, 'iou': iou})\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_accuracy, test_dice, test_iou = model.evaluate(test_dataset)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "print(f\"Test Dice Coefficient: {test_dice}\")\n",
        "print(f\"Test IoU: {test_iou}\")"
      ],
      "metadata": {
        "id": "wgHnQ4vABRon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtWVTrAeBRon"
      },
      "outputs": [],
      "source": [
        "def plot_predictions(dataset, model, num_examples=5):\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    for images, masks in dataset.take(1):\n",
        "        preds = model.predict(images)\n",
        "\n",
        "        for i in range(num_examples):\n",
        "            plt.subplot(3, num_examples, i + 1)\n",
        "            plt.imshow(images[i])\n",
        "            plt.title('Image')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(3, num_examples, num_examples + i + 1)\n",
        "            plt.imshow(masks[i, :, :], cmap='gray')\n",
        "            plt.title('True Mask')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(3, num_examples, 2 * num_examples + i + 1)\n",
        "            plt.imshow(preds[i].squeeze(), cmap='gray')\n",
        "            plt.title('Predicted Mask')\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot predictions\n",
        "plot_predictions(test_dataset, model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1ndiwxCG1XgfsN4RSsclApuXuQLihxKNZ"
      ],
      "metadata": {
        "id": "jPx1RSJqBRon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq surprise.zip"
      ],
      "metadata": {
        "id": "8pMqRluCBRon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_images, eval_masks = get_file_paths('surprise', [''])"
      ],
      "metadata": {
        "id": "-fKp9UPdBRon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = tf.data.Dataset.from_tensor_slices((eval_images, eval_masks))\n",
        "test_dataset = test_dataset.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(batch_size)\n",
        "test_dataset = test_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "vVaGijwpBRon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_accuracy, test_dice, test_iou = model.evaluate(test_dataset)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "print(f\"Test Dice Coefficient: {test_dice}\")\n",
        "print(f\"Test IoU: {test_iou}\")"
      ],
      "metadata": {
        "id": "vlF1VYXMBRon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation of Trained Models"
      ],
      "metadata": {
        "id": "pk5Q1WcJJs-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1abQH_9ET6CRzzZ5YAysOv-NLH4tCv7Vf"
      ],
      "metadata": {
        "id": "NzScPxQ6RHfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1vbzH_BlrZ_dAlAeg0VZMasriMxI0RQCJ"
      ],
      "metadata": {
        "id": "jWa-GAl3P_8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model (assuming it's saved as 'model-sdc-seg-v2_192.h5')\n",
        "model_unet = tf.keras.models.load_model('model-192-unet.h5', custom_objects={'dice_coefficient': dice_coefficient, 'iou': iou})\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_accuracy, test_dice, test_iou = model_unet.evaluate(test_dataset)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "print(f\"Test Dice Coefficient: {test_dice}\")\n",
        "print(f\"Test IoU: {test_iou}\")"
      ],
      "metadata": {
        "id": "jpvR_vLYJu85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model (assuming it's saved as 'model-sdc-seg-v2_192.h5')\n",
        "model_segnet = tf.keras.models.load_model('model-192-segnet.h5', custom_objects={\n",
        "    'MaxPoolingWithArgmax2D': MaxPoolingWithArgmax2D,\n",
        "    'MaxUnPooling2D': MaxUnPooling2D,\n",
        "    'dice_coefficient': dice_coefficient,\n",
        "    'iou': iou\n",
        "})\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_accuracy, test_dice, test_iou = model_segnet.evaluate(test_dataset)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "print(f\"Test Dice Coefficient: {test_dice}\")\n",
        "print(f\"Test IoU: {test_iou}\")"
      ],
      "metadata": {
        "id": "zq-aQ1WbMRYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_images, eval_masks = get_file_paths('surprise', [''])"
      ],
      "metadata": {
        "id": "aq6g9evISLyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = tf.data.Dataset.from_tensor_slices((eval_images, eval_masks))\n",
        "eval_dataset = eval_dataset.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "eval_dataset = eval_dataset.batch(batch_size)\n",
        "eval_dataset = eval_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "9aDX1UvWSLyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_accuracy, test_dice, test_iou = model_unet.evaluate(eval_dataset)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Eval Loss: {test_loss}\")\n",
        "print(f\"Eval Accuracy: {test_accuracy}\")\n",
        "print(f\"Eval Dice Coefficient: {test_dice}\")\n",
        "print(f\"Eval IoU: {test_iou}\")"
      ],
      "metadata": {
        "id": "tHG-xLyBST43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_accuracy, test_dice, test_iou = model_segnet.evaluate(eval_dataset)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Eval Loss: {test_loss}\")\n",
        "print(f\"Eval Accuracy: {test_accuracy}\")\n",
        "print(f\"Eval Dice Coefficient: {test_dice}\")\n",
        "print(f\"Eval IoU: {test_iou}\")"
      ],
      "metadata": {
        "id": "EDC4SavPScke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(dataset, model, num_examples=5):\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    for images, masks in dataset.take(1):\n",
        "        preds = model.predict(images)\n",
        "\n",
        "        for i in range(num_examples):\n",
        "            plt.subplot(3, num_examples, i + 1)\n",
        "            plt.imshow(images[i])\n",
        "            plt.title('Image')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(3, num_examples, num_examples + i + 1)\n",
        "            plt.imshow(masks[i, :, :], cmap='gray')\n",
        "            plt.title('True Mask')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(3, num_examples, 2 * num_examples + i + 1)\n",
        "            plt.imshow(preds[i].squeeze(), cmap='gray')\n",
        "            plt.title('Predicted Mask')\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "k0Gl1xLWS4ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot predictions\n",
        "plot_predictions(eval_dataset, model_unet)"
      ],
      "metadata": {
        "id": "jHvIK0QXS7JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot predictions\n",
        "plot_predictions(eval_dataset, model_segnet)"
      ],
      "metadata": {
        "id": "DS-AyR0XTDa3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AxTl2Q0yxwJj",
        "72zTCo0xBRof",
        "uawn1Ff0BRol"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}